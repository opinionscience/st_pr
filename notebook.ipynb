{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2024-05-15 09:27:35.738063: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 09:27:36.190977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 09:27:37.621313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from opsci_toolbox.helpers.common import load_pickle, write_pickle, write_json\n",
    "from opsci_toolbox.helpers.dataviz import generate_hexadecimal_color_palette\n",
    "from tqdm import tqdm\n",
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import re\n",
    "from opsci_toolbox.helpers.nlp import *\n",
    "\n",
    "## Fonction intermédiaire pour générer les clés de jointure\n",
    "def generate_index(df, col_author_id ='author_id', col_date='created_time'):\n",
    "    res=[]\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"generation des index\"): \n",
    "        new_index=\".\".join([ str(i) for i in [ row[col_author_id], row[col_date].year, row[col_date].month, row[col_date].day]])\n",
    "        res.append(new_index)\n",
    "    df[\"index\"]=res\n",
    "    \n",
    "    return df\n",
    "                     \n",
    "def avg_performance(df, \n",
    "                    col_date='created_time', \n",
    "                    col_author_id='author_id', \n",
    "                    col_engagement=['shares', 'comments', 'reactions', 'likes','top_comments', 'love', 'wow', 'haha', \n",
    "                                    'sad', 'angry','total_engagement', 'replies', 'percentage_replies'], \n",
    "                    rolling_period='7D'):\n",
    "                     \n",
    "    # Nettoyage au cas où\n",
    "    df[col_date] = pd.to_datetime(df[col_date]) \n",
    "    df = df.sort_values([col_author_id, col_date]) \n",
    "\n",
    "    # Le point central c'est la colone created_time, on la met en index.\n",
    "    # Ensuite on groupe par author_id en gardant les colonnes de valeurs.\n",
    "    # On applique la moyenne mean sur un rolling tous les 2 jours. Automatiquement il va prendre l'index, ici created_time comme pivot. \n",
    "    # On met tout à plat\n",
    "    average = df.set_index(col_date).groupby(col_author_id)[col_engagement].rolling(rolling_period).mean(numeric_only=True).reset_index()\n",
    "                     \n",
    "    # Sur les résultats précédent, on simplifie pour récupérer une liste avec juste la liste jour / author_id\n",
    "    average = average.set_index(col_date).groupby([col_author_id]).resample('1D').last(numeric_only=True).reset_index()\n",
    "\n",
    "    # On génère nos supers index\n",
    "    df=generate_index(df, col_author_id =col_author_id, col_date=col_date)    \n",
    "    \n",
    "    average = generate_index(average, col_author_id = col_author_id, col_date=col_date)\n",
    "\n",
    "    # On fusionne \n",
    "    df = pd.merge(df, average[['index']+col_engagement], how='left', on=['index'], suffixes=('', '_avg'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def kpi_reaction(df, cols):\n",
    "    \"\"\"\n",
    "    Cette fonction prend un dataframe et une liste de colonnes en entrée.\n",
    "    Pour chaque colonne, on va calculer le taux de sur-réaction.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df['tx_'+col]=(df[col]-df[col+'_avg'])/(df[col]+df[col+'_avg'])\n",
    "    return df\n",
    "\n",
    "def get_reactions_type(df, cols, col_dest):\n",
    "    all_val=[]\n",
    "    \n",
    "    for i,row in tqdm(df.iterrows(), total=df.shape[0], desc=\"qualification des posts\"):\n",
    "        \n",
    "        str_val=''\n",
    "        count=0\n",
    "        for col in cols:\n",
    "            if row[col]>0:\n",
    "                str_val=str_val+' '+col.replace('tx_', 'sur-')\n",
    "                count=count+1\n",
    "        if count==0:\n",
    "            str_val=\"sous reaction\"\n",
    "        if count==len(cols):\n",
    "            str_val=\"sur reaction totale\"\n",
    "            \n",
    "        all_val.append(str_val.strip())\n",
    "            \n",
    "    df[col_dest]=all_val       \n",
    "    return df\n",
    "\n",
    "def remove_brackets(text, replacement=\"\"):\n",
    "    # pattern = r'\\b(?:train|ter|eurostar|t[h]?al[iy]s|tgv|intercité[s]?|transilien[s]?)*\\s*?(n[°]?|num[ée]ro[s]?|num)?\\s*?\\d+\\b'\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    result = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_pickle(\"/home/erwan/scripts/st_pr_v2/data/df_prod_v2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opsci_toolbox.helpers.common import load_pickle\n",
    "\n",
    "d = load_pickle(\"/home/erwan/scripts/st_pr_v2/data/df_prod_v2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/erwan/scripts/st_pr_v2/data/df_prod_v2.pickle'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opsci_toolbox.helpers.common import write_pickle\n",
    "\n",
    "\n",
    "write_pickle(df, \"/home/erwan/scripts/st_pr_v2/data\", \"df_prod_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_user_id = \"origin_id1\"\n",
    "# col_post_id = \"uniq_id\"\n",
    "col_date = \"message_date\"\n",
    "\n",
    "\n",
    "cols_sureaction_metrics = ['views', 'engagements', 'total_reactions', 'replies_count', 'forwards']\n",
    "\n",
    "cols_typologie_sureaction=['views', \"total_reactions\", \"forwards\"]\n",
    "\n",
    "rolling_period_sureaction = '7D'\n",
    "\n",
    "start_date = datetime(2024, 1, 1, tzinfo=pytz.UTC) \n",
    "end_date = datetime(2024, 4, 1, tzinfo=pytz.UTC)\n",
    "\n",
    "spacy_lang = \"en\"                                       #language of the stopwords\n",
    "spacy_model = \"en_core_web_lg\"                         # spacy model to import : ru_core_news_lg, en_core_web_lg, fr_core_news_lg\n",
    "pos_to_keep = [\"VERB\",\"NOUN\",\"ADJ\", \"ADV\", \"PROPN\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_pickle(\"data/df.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"]=df[\"datetime\"].dt.date\n",
    "\n",
    "df= avg_performance(\n",
    "    df, \n",
    "    col_date=col_date, \n",
    "    col_author_id=\"channel\", \n",
    "    col_engagement= cols_sureaction_metrics, \n",
    "    rolling_period=rolling_period_sureaction\n",
    "    ) \n",
    "\n",
    "# on calcule les taux de sur-réaction pour notre liste de métriques\n",
    "df=kpi_reaction(df, cols_sureaction_metrics)\n",
    "cols_tx_engagement=['tx_'+c for c in cols_sureaction_metrics]\n",
    "df[cols_tx_engagement]=df[cols_tx_engagement].fillna(-1)\n",
    "\n",
    "\n",
    "# on supprime nos colonnes contenant la performance moyenne (on ne devrait plus en avoir besoin)\n",
    "cols_to_drop = [c for c in df.columns if c.lower()[-4:] == '_avg']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# on catégorise les formes de réaction\n",
    "cols_typologie = [\"tx_\"+ col for col in cols_typologie_sureaction]\n",
    "df=get_reactions_type(df, cols_typologie, 'type_engagement')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT PRECLEANING\n",
      "NLP PROCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLP Process: 100%|██████████| 49641/49641 [07:04<00:00, 117.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df_stopwords = load_stopwords_df(spacy_lang)\n",
    "stopwords = df_stopwords['word'].to_list()\n",
    "\n",
    "nlp = load_spacy_model(spacy_model,  disable_components=[\"transformer\", \"trainable_lemmatizer\", \"textcat_multilabel\", \"textcat\", \"entity_ruler\", \"entity_linker\"], lang_detect=False, emoji=True)\n",
    "\n",
    "# basic precleaning of text \n",
    "print(\"TEXT PRECLEANING\")\n",
    "df = TM_clean_text(df, \"translated_text\", \"clean_text\")\n",
    "\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_brackets)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_extra_spaces)\n",
    "\n",
    "# lemmatize text, remove stop words and keep only some PoS\n",
    "print(\"NLP PROCESS\")\n",
    "df = TM_nlp_process(nlp, df, \"clean_text\", \"lemmatized_text\", pos_to_keep, stopwords, batch_size=100, n_process=1, stats=False, join_list = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/reaction_color_palette.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reaction_color_palette = generate_hexadecimal_color_palette(df[\"type_engagement\"].unique())\n",
    "channel_color_palette = generate_hexadecimal_color_palette(df[\"channel\"].unique())\n",
    "\n",
    "df[\"channel_color\"]=df[\"channel\"].map(channel_color_palette)\n",
    "df[\"surreaction_color\"]=df[\"type_engagement\"].map(reaction_color_palette)\n",
    "write_json(channel_color_palette, \"data\", \"channel_color_palette\")\n",
    "write_json(reaction_color_palette, \"data\", \"reaction_color_palette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/df_prod_chroma.pickle'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = \"BAAI/bge-m3\"                 #\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"BAAI/bge-m3\",  \"DeepPavlov/rubert-base-cased\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {\"batch_size\":2}\n",
    "\n",
    "HF_encoder = load_HF_embeddings(embedding_model, encode_kwargs, model_kwargs)\n",
    "\n",
    "embeddings = HF_vectorize(HF_encoder, df[\"translated_text\"])\n",
    "\n",
    "df['embeddings'] = embeddings\n",
    "\n",
    "cols_to_keep = ['origin_id1', 'channel_id', \"channel\", \"channel_description\", 'message_id', \"uniq_id\", \"message_date\", \"date\", \"datetime\", \"text\", \"translated_text\", \n",
    "                \"is_reply\", 'views', 'forwards', 'replies_count', 'total_reactions', \"engagements\", \"sentiment\", 'tx_views',\n",
    "                'tx_engagements', 'tx_total_reactions', 'tx_replies_count',\n",
    "                'tx_forwards', 'type_engagement', \"channel_color\", \"surreaction_color\", \"lemmatized_text\", \"embeddings\"]\n",
    "\n",
    "\n",
    "df=df[cols_to_keep]\n",
    "\n",
    "write_pickle(df, \"data\", \"df_prod_chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/.local/lib/python3.10/site-packages/opsci_toolbox/helpers/nlp.py:763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NER_type'] = None\n",
      "/home/erwan/.local/lib/python3.10/site-packages/opsci_toolbox/helpers/nlp.py:764: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NER_text'] = None\n",
      "/home/erwan/.local/lib/python3.10/site-packages/opsci_toolbox/helpers/nlp.py:765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NER_start_char'] = None\n",
      "/home/erwan/.local/lib/python3.10/site-packages/opsci_toolbox/helpers/nlp.py:766: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['NER_end_char'] = None\n"
     ]
    }
   ],
   "source": [
    "from opsci_toolbox.helpers.nlp import spacy_NER\n",
    "spacy_lang = \"ru\"                                       #language of the stopwords\n",
    "spacy_model = \"ru_core_news_lg\"                         # spacy model to import : ru_core_news_lg, en_core_web_lg, fr_core_news_lg\n",
    "pos_to_keep = [\"VERB\",\"NOUN\",\"ADJ\", \"ADV\", \"PROPN\"] \n",
    "\n",
    "nlp = load_spacy_model(spacy_model,  disable_components=[\"transformer\", \"trainable_lemmatizer\", \"textcat_multilabel\", \"textcat\", \"entity_ruler\", \"entity_linker\"], lang_detect=False, emoji=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "df_ner = spacy_NER(nlp, df, \"text\", entities_to_keep=['PER','ORG', 'LOC'], explode= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"/home/erwan/scripts/bertopic/chroma\")\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = \"BAAI/bge-m3\"                 #\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"BAAI/bge-m3\",  \"DeepPavlov/rubert-base-cased\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'batch_size':32}\n",
    "\n",
    "df[\"date\"] = df[\"datetime\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "cols_metadata = ['origin_id1', 'channel_id', \"channel\", 'message_id', \"date\", \"is_reply\", 'views', 'forwards', 'replies_count', 'total_reactions', \"engagements\", \"sentiment\", 'tx_views', 'tx_engagements', 'tx_total_reactions', 'tx_replies_count', 'tx_forwards', 'type_engagement', \"translated_text\"]\n",
    "col_text = \"text\"\n",
    "col_id = \"uniq_id\"\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "\n",
    "df_filter = df.sample(10)\n",
    "lst_text = list(df_filter[col_text])\n",
    "lst_ids = list(df_filter[col_id])\n",
    "metadatas =  df_filter[cols_metadata].to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=lst_text,\n",
    "    metadatas=metadatas,\n",
    "    ids=lst_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
